{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\afhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import gensim\n",
    "import spacy\n",
    "import altair as alt\n",
    "from hyphen import Hyphenator\n",
    "from gensim.parsing.preprocessing import preprocess_string, STOPWORDS \n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.metrics import accuracy_score\n",
    "import textstat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "cwd = os.getcwd()\n",
    "from sklearn.svm import SVC\n",
    "from operator import itemgetter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSTALLED VERSIONS\n",
      "------------------\n",
      "commit           : db08276bc116c438d3fdee492026f8223584c477\n",
      "python           : 3.7.9.final.0\n",
      "python-bits      : 64\n",
      "OS               : Windows\n",
      "OS-release       : 10\n",
      "Version          : 10.0.19041\n",
      "machine          : AMD64\n",
      "processor        : AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD\n",
      "byteorder        : little\n",
      "LC_ALL           : None\n",
      "LANG             : None\n",
      "LOCALE           : None.None\n",
      "\n",
      "pandas           : 1.1.3\n",
      "numpy            : 1.17.0\n",
      "pytz             : 2020.1\n",
      "dateutil         : 2.8.1\n",
      "pip              : 20.2.4\n",
      "setuptools       : 50.3.1.post20201107\n",
      "Cython           : 0.29.21\n",
      "pytest           : 6.1.1\n",
      "hypothesis       : None\n",
      "sphinx           : 3.2.1\n",
      "blosc            : None\n",
      "feather          : None\n",
      "xlsxwriter       : 1.3.7\n",
      "lxml.etree       : 4.6.1\n",
      "html5lib         : 1.1\n",
      "pymysql          : None\n",
      "psycopg2         : None\n",
      "jinja2           : 2.11.2\n",
      "IPython          : 7.19.0\n",
      "pandas_datareader: None\n",
      "bs4              : 4.9.3\n",
      "bottleneck       : 1.3.2\n",
      "fsspec           : 0.8.3\n",
      "fastparquet      : None\n",
      "gcsfs            : None\n",
      "matplotlib       : 3.3.2\n",
      "numexpr          : 2.7.1\n",
      "odfpy            : None\n",
      "openpyxl         : 3.0.5\n",
      "pandas_gbq       : None\n",
      "pyarrow          : None\n",
      "pytables         : None\n",
      "pyxlsb           : None\n",
      "s3fs             : None\n",
      "scipy            : 1.5.2\n",
      "sqlalchemy       : 1.3.20\n",
      "tables           : 3.6.1\n",
      "tabulate         : None\n",
      "xarray           : None\n",
      "xlrd             : 1.2.0\n",
      "xlwt             : 1.3.0\n",
      "numba            : 0.51.2\n"
     ]
    }
   ],
   "source": [
    "pd.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep2 = pd.read_pickle('brosko_train_f.pkl')\n",
    "df_tokenized = pd.read_pickle('df_tokenized')\n",
    "p_list = ['nouns','verbs', 'Pnouns', 'adjectives', 'adverbs']\n",
    "for p in p_list:\n",
    "    col = p + '_%'\n",
    "    df_tokenized[col] = df_tokenized[p].astype(float)/df_tokenized['num_words'].astype(float)\n",
    "merged = df_tokenized.merge(prep2, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_text_x', 'label_x', 'tokens', 'pos', 'nouns', 'verbs',\n",
       "       'Pnouns', 'adjectives', 'adverbs', 'num_words', 'syls', 'max_syls',\n",
       "       'avg_syls', 'std_syls', 'flesch_score', 'flesch_grade_lvl',\n",
       "       'fog_grade_lvl', 'ARI_grade', 'CLI_grade', 'LWF_grade',\n",
       "       'Dale-Chall_score', 'combined_grade', 'nouns_%', 'verbs_%', 'Pnouns_%',\n",
       "       'adjectives_%', 'adverbs_%', 'index', 'original_text_y', 'label_y',\n",
       "       'clean_text', 'word_count', 'clean_text_no_stop', 'word_count_no_stop',\n",
       "       'Nsyll', 'AoA_Kup_lem', 'Perc_known_lem'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the features and scale the data\n",
    "cols = ['nouns', 'verbs',\n",
    "        'Pnouns', 'adjectives', 'adverbs', 'num_words', 'max_syls',\n",
    "        'nouns_%', 'verbs_%', 'Pnouns_%','adjectives_%', 'adverbs_%',\n",
    "        'avg_syls', 'std_syls', 'flesch_score', 'flesch_grade_lvl',\n",
    "        'fog_grade_lvl', 'ARI_grade', 'CLI_grade', 'LWF_grade',\n",
    "        'Dale-Chall_score','word_count_no_stop','Nsyll','AoA_Kup_lem','Perc_known_lem']\n",
    "X_train = merged[cols]\n",
    "#X_train = merged[['flesch_score', 'flesch_grade_lvl',\n",
    "#       'fog_grade_lvl', 'ARI_grade', 'CLI_grade', 'LWF_grade',\n",
    "#       'Dale-Chall_score']]\n",
    "X_train = X_train.replace(np.nan, 0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "y_train = merged['label_x'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "X= X_train\n",
    "y = y_train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train_df = pd.DataFrame(X_train, columns = cols)\n",
    "X_test_df = pd.DataFrame(X_test, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "estimator_SVM = SGDClassifier()\n",
    "params = {\n",
    "    'loss':['hinge'],\n",
    "    'penalty': ['l2','l1'],\n",
    "    'alpha': [0.0001,0.001,0.1],\n",
    "\n",
    "}\n",
    "#use grid search for tuning\n",
    "grid_search_SVM = GridSearchCV(\n",
    "    estimator=estimator_SVM,\n",
    "    param_grid=params,\n",
    "    scoring = 'accuracy',\n",
    "    n_jobs = -1,\n",
    "    cv = 5,\n",
    "    verbose=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  30 | elapsed:   10.0s remaining:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  30 | elapsed:   11.7s remaining:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   13.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 done\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  30 | elapsed:    3.2s remaining:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  30 | elapsed:    4.2s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 done\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  30 | elapsed:    3.8s remaining:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  30 | elapsed:    5.3s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    5.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 done\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  30 | elapsed:    2.8s remaining:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  30 | elapsed:    3.9s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    4.3s finished\n"
     ]
    }
   ],
   "source": [
    "#develop a couple different feature sets\n",
    "X_train_1 = X_train_df.values #full\n",
    "X_train_2 = X_train_df[['flesch_score', 'flesch_grade_lvl','fog_grade_lvl', 'ARI_grade', 'CLI_grade', 'LWF_grade','Dale-Chall_score']].values #just other scorers\n",
    "X_train_3 = X_train_df[['num_words', 'max_syls','nouns_%', 'verbs_%', 'Pnouns_%','adjectives_%', 'adverbs_%','avg_syls', 'std_syls','AoA_Kup_lem','Perc_known_lem']].values #just text stats\n",
    "X_train_4 = X_train_df[['AoA_Kup_lem','Perc_known_lem','word_count_no_stop' ]].values #basic model\n",
    "\n",
    "X_test_1 = X_test_df.values #full\n",
    "X_test_2 = X_test_df[['flesch_score', 'flesch_grade_lvl','fog_grade_lvl', 'ARI_grade', 'CLI_grade', 'LWF_grade','Dale-Chall_score']].values #just other scorers\n",
    "X_test_3 = X_test_df[['num_words', 'max_syls','nouns_%', 'verbs_%', 'Pnouns_%','adjectives_%', 'adverbs_%','avg_syls', 'std_syls','AoA_Kup_lem','Perc_known_lem']].values #just text stats\n",
    "X_test_4 = X_test_df[['AoA_Kup_lem','Perc_known_lem','word_count_no_stop' ]].values #basic model\n",
    "\n",
    "#Do the search\n",
    "SVM_1=grid_search_SVM.fit(X_train_1, y_train) #full feature list\n",
    "y_pred_SVM =SVM_1.predict(X_test_1)\n",
    "print('1 done')\n",
    "SVM_2=grid_search_SVM.fit(X_train_2, y_train) #just other scoring methodsI w\n",
    "y_pred_SVM2 =SVM_2.predict(X_test_2)\n",
    "print('2 done')\n",
    "SVM_3=grid_search_SVM.fit(X_train_3, y_train) #just text stats\n",
    "y_pred_SVM3 =SVM_3.predict(X_test_3)\n",
    "print('3 done')\n",
    "SVM_4=grid_search_SVM.fit(X_train_4, y_train) #basic model, probably likes starbucks and uggs\n",
    "y_pred_SVM4 =SVM_4.predict(X_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.001, 'loss': 'hinge', 'penalty': 'l1'}\n",
      "Best Score - SVM: 0.6270547390337075\n"
     ]
    }
   ],
   "source": [
    "#Parameter setting that gave the best results on the hold out data.\n",
    "print(grid_search_SVM.best_params_ ) \n",
    "#Mean cross-validated score of the best_estimator\n",
    "print('Best Score - SVM:', grid_search_SVM.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score - KNN: 0.6684393827485577\n"
     ]
    }
   ],
   "source": [
    "print('Best Score - KNN:', KNN_1.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score - SVM - 1: 0.6622798726133174\n",
      "Accuracy Score - SVM - 2: 0.645949365247866\n",
      "Accuracy Score - SVM - 3: 0.6314293192955923\n",
      "Accuracy Score - SVM - 4: 0.6239402620442945\n"
     ]
    }
   ],
   "source": [
    "#check with the hold out data\n",
    "print('Accuracy Score - SVM - 1:', metrics.accuracy_score(y_test, y_pred_SVM)) \n",
    "print('Accuracy Score - SVM - 2:', metrics.accuracy_score(y_test, y_pred_SVM2)) \n",
    "print('Accuracy Score - SVM - 3:', metrics.accuracy_score(y_test, y_pred_SVM3)) \n",
    "print('Accuracy Score - SVM - 4:', metrics.accuracy_score(y_test, y_pred_SVM4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the actual test data for submission\n",
    "submit_df = pd.read_pickle('test_data.pkl')\n",
    "p_list = ['nouns','verbs', 'Pnouns', 'adjectives', 'adverbs']\n",
    "for p in p_list:\n",
    "    col = p + '_%'\n",
    "    submit_df[col] = submit_df[p].astype(float)/submit_df['num_words'].astype(float)\n",
    "submit_df = submit_df.replace(np.nan, 0)\n",
    "submit_s = scaler.transform(submit_df[cols])\n",
    "submit = pd.DataFrame(submit_s, columns = cols)\n",
    "X_sub_1 = submit.values #full\n",
    "X_sub_2 = submit[['flesch_score', 'flesch_grade_lvl','fog_grade_lvl', 'ARI_grade', 'CLI_grade', 'LWF_grade','Dale-Chall_score']].values #just other scorers\n",
    "X_sub_3 = submit[['num_words', 'max_syls','nouns_%', 'verbs_%', 'Pnouns_%','adjectives_%', 'adverbs_%','avg_syls', 'std_syls','AoA_Kup_lem','Perc_known_lem']].values #just text stats\n",
    "X_sub_4 = submit[['AoA_Kup_lem','Perc_known_lem','word_count_no_stop' ]].values #basic model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the best model\n",
    "knn_f = KNeighborsClassifier(leaf_size= 20, metric= 'chebyshev', n_neighbors=1000, p= 1, weights= 'distance') #start with square root of N, to add a loop later\n",
    "knn_f.fit(X_train_1, y_train)\n",
    "y_pred = knn_f.predict(X_sub_1)\n",
    "df = pd.DataFrame()\n",
    "df['id'] =submit_df.index\n",
    "df['label'] = y_pred\n",
    "df.to_csv('knn_textscores_only.csv',index = False)\n",
    "# 0.71537 on kaggle! passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the best \"small\" model\n",
    "knn_f_s = KNeighborsClassifier(leaf_size= 20, metric= 'chebyshev', n_neighbors=1000, p= 1, weights= 'distance') #start with square root of N, to add a loop later\n",
    "knn_f_s.fit(X_train_2, y_train)\n",
    "y_pred = knn_f_s.predict(X_sub_2)\n",
    "df = pd.DataFrame()\n",
    "df['id'] =submit_df.index\n",
    "df['label'] = y_pred\n",
    "df.to_csv('knn_textscores_only.csv',index = False)\n",
    "#0.70100, barely worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = KNN_1.predict(X_sub_4)\n",
    "df = pd.DataFrame()\n",
    "df['id'] =submit_df.index\n",
    "df['label'] = y_pred\n",
    "df.to_csv('knn_textscores_only.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119092, 7)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sub_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
